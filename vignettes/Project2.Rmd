---
title: "Project2"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Project2}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(Project2Caba0009)
```

# Introduction
In this project we were asked perform several analyses from the book. These analyses included checking for multivariate normality as well as carrying out a box cox transformation, a Hotellings T square test, a MLR, and a PCA.

# Analyses
This section will include all the aforementioned analysis techniques in order.

## Multivariate Normality
Here we are checking for multivariate normality. We will do so by first creating a Chi-Square plot of the original data, performing a box cox transformation to transform the data to a more normal state, and successively creating a Chi-Square plot of the transformed data for comparison. The function created will also output the optimal lambda values used for transforming each column of the data as well as outputting the transformed data.

```{r, fig.align='center', fig.width=8, fig.height=4}
data <- T4_3
print("Original Data")
print(data)
output <- mvNormCheck(data)
print(output$optimal_lambdas)
print(output$transformed_data)
```

We can clearly see from the compared Chi-Square plots that we were able to reduce the non normality of some of the outliers in the original data using the box cox transformation. However, the furthest outlier was unable to be reduced as the lambda values are calculated for the data column as a whole and not specifically to any individual outlier. Yet, it is encouraging to see the positive changes towards normality for the data sans the farthest outlying point.

## Hotellings T square test
This method will follow example 5.2 from the book.

```{r, fig.align='center', fig.height=15, fig.width=5}
data <- T5_1
H0 <- c(4,50,10)
alpha <- 0.10
results <- HotellingTSq(data, H0, alpha)
print(results)
```

The example above follows Example 5.2 from the book about the sweat data. Comparing with the results in the book we see agreement in both the $T^2$ value of 9.74 based on the hypothesize mean vector [4,50,10] and the provided data as well as the critical value of ~8.18. This function also presents the confidence ellipse plots for each pairing of variables which appropriately captures the majority of the data confirming the validity of the Hotelling's $T^2$ test for this data set.

## Multivariate Linear Regression
To make a multivariate linear regression for data, we utilized the lm() function alongside generalized code that is easily adapted to a range of data set sizes and complexities. This method will follow example 7.4 from the book.

```{r, fig.align='center', fig.height=6, fig.width=6}
data <- T7_1
target_index = 3
ind_indices = c(1,2)
results <- mvReg(data, target_index=target_index, ind_index=ind_indices)

print("Coefficients of fitted model")
print(results$coefficients)

print("Standard deviations of coefficients")
print(results$standardDev)

print("R Squared of model")
print(results$rSquared)

print("Full fitted model")
print(results$fitted_model)
```

The coefficients found for the model result in a fitted equation of $\hat{y} = 30.967 + 2.634z_1 + 0.045z_2$ matching that of the results seen in the book. The estimated standard deviations of the least squares coefficients are also shown matching the book's results.The $R^2$ of 0.834 also agrees with the book. Further, we can see from the fitted vs observed values plot that we have general agreement between the two axes increasing our confidence in the model.

## Principle Component Analysis
For this principle component analysis function, I have elected to derive all the necessary information from scratch instead of using built in functions to further my understanding of the methods behind the analysis. This method will follow example 8.3 from the book.

```{r, fig.align='center', fig.height=6, fig.width=6}
data <- T8_5
results <- PCA(data)

print("Coefficients for the Principal Components")
print(results$coefficients)

print("Variances of the Principle Components")
print(results$variances)

print("Cumulative percentage of total variance")
print(results$cumulative)

print("Scree Plot")
print(results$plot)

```

We can clearly see the agreement between the results presented here and those in the book. Additionally, the scree plot shows the cumulative variance according to each successive principal component. This is helpful for visualizing the amount that each principle component is contributing to the total variance of data.
